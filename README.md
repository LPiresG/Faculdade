# Scale Dot-Product Attention

Este projeto implementa manualmente o mecanismo de **Scaled Dot-Product Attention**.
A implementaÃ§Ã£o foi feita usando a biblioteca NumPy.

Esse trabalho coloca em codigo a formula da atenÃ§Ã£o:
Attention(Q, K, V) = softmax((Q Â· Káµ€) / âˆšdk) Â· V

Onde:
Q = Query
K = Key
V =Value
dk = dimenÃ§Ã£o das chaves

## NormalizaÃ§Ã£o (âˆšdk):

Primeiro Ã© calculado o produto escalar de Q pala a matriz transposta de K, em seguida divide o resultado pela raiz quadrada de dk, onde dk Ã© a dimensÃ£o dos vetores de chave (Key).
Quando a dimensÃ£o ğ‘‘k Ã© grande, os valores do produto escalar tendem a crescer muito.
Isso pode fazer com que o softmax gere valores extremamente altos ou muito prÃ³ximos de zero.

Em resumo a normalizaÃ§Ã£o por âˆšğ‘‘k foi aplicada dividindo o produto escalar Q Â· Káµ€ por âˆšdk para evitar valores muito grandes e garantir estabilidade numÃ©rica no softmax.

## Exemplo uso e de input e o output esperado:

import numpy as np

Q = np.array([[1, 0], [0, 1]])
K = np.array([[1, 0], [0, 1]])
V = np.array([[1, 2], [3, 4]])

result = ScaleDotProductAttention()
attention, softmax_result = result.attentionFormula(Q, K, V)

print("SaÃ­da:", attention)
print("Softmax:", softmax_result)

# Output:

SaÃ­da:
[[1.6604769  2.6604769 ]
 [2.3395231  3.3395231 ]]

Resultado do softmax:
[[0.66976155 0.33023845]
 [0.33023845 0.66976155]]
